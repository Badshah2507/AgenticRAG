{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8423ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U langchain langgraph langchain_openai elasticsearch streamlit langchain-community sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f78f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY:········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(key: str):\n",
    "    if key not in os.environ:\n",
    "        os.environ[key] = getpass.getpass(f\"{key}:\")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ed95eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from elasticsearch import Elasticsearch\n",
    "import pandas as pd\n",
    "\n",
    "# Connect to ElasticSearch\n",
    "es_client = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Initialize embeddingd\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load data in dfs\n",
    "claims_df = pd.read_csv(\"claims.csv\")\n",
    "mainframe_df = pd.read_csv(\"mainframe_data.csv\")\n",
    "vsps_df = pd.read_csv(\"vsps.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ff4cb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DataFrameLoader\n",
    "\n",
    "# Process data of claims\n",
    "doc_loader_claims = DataFrameLoader(claims_df, page_content_column=\"Claim_ID\")\n",
    "claims_documents = doc_loader_claims.load()\n",
    "claims_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "claims_docs = claims_text_splitter.split_documents(claims_documents)\n",
    "\n",
    "# Process data of mainframe\n",
    "doc_loader_mainframe = DataFrameLoader(mainframe_df, page_content_column=\"Transaction_ID\")\n",
    "mainframe_documents = doc_loader_mainframe.load()\n",
    "mainframe_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "mainframe_docs = mainframe_text_splitter.split_documents(mainframe_documents)\n",
    "\n",
    "# Process data of vsps\n",
    "doc_loader_vsps = DataFrameLoader(vsps_df, page_content_column=\"Transaction_ID\")\n",
    "vsps_documents = doc_loader_vsps.load()\n",
    "vsps_text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "vsps_docs = vsps_text_splitter.split_documents(vsps_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a04006e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prathamesh Bonde\\AppData\\Local\\Temp\\ipykernel_36224\\3354538367.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import ElasticsearchStore\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load an open-source embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "index_claims = ElasticsearchStore.from_documents(claims_docs, index_name=\"claims\", embedding=embedding_model, es_connection=es_client)\n",
    "\n",
    "\n",
    "\n",
    "# Load elasticsearch indices\n",
    "# index_claims = ElasticsearchStore.from_documents(claims_docs, index_name=\"claims\", embedding=embeddings, es_connection=es_client)\n",
    "# index_visa_stop_payment = ElasticsearchStore.from_documents(vsps_docs, index_name=\"visa_stop_payment\", embedding=embeddings, es_connection=es_client)\n",
    "# index_mainframe = ElasticsearchStore.from_documents(mainframe_docs, index_name=\"mainframe_transactions\", embedding=embeddings, es_connection=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc0103e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def format_metadata(metadata):\n",
    "    if \"Placement_Date\" in metadata:\n",
    "        date_value = metadata[\"Placement_Date\"]\n",
    "        \n",
    "        # Replace \"-\" with None or \"N/A\"\n",
    "        if date_value == \"-\":\n",
    "            metadata[\"Placement_Date\"] = None  # Or use \"N/A\" if storing as string\n",
    "        else:\n",
    "            try:\n",
    "                # Convert to ISO format (YYYY-MM-DD) if valid\n",
    "                metadata[\"Placement_Date\"] = datetime.strptime(date_value, \"%d-%m-%Y\").strftime(\"%Y-%m-%d\")\n",
    "            except ValueError:\n",
    "                print(f\"Skipping invalid date format: {date_value}\")\n",
    "                metadata[\"Placement_Date\"] = None  # Handle unexpected formats\n",
    "                \n",
    "    return metadata\n",
    "\n",
    "# Apply this function to each document\n",
    "for doc in vsps_docs:\n",
    "    doc.metadata = format_metadata(doc.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a4eacd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_visa_stop_payment = ElasticsearchStore.from_documents(vsps_docs, index_name=\"visa_stop_payment\", embedding=embedding_model, es_connection=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "814b5091",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_mainframe = ElasticsearchStore.from_documents(mainframe_docs, index_name=\"mainframe_transactions\", embedding=embedding_model, es_connection=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ff70336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_visa_stop_payment.similarity_search(\"TXN002\", k=1)[0].metadata.get(\"stop_payment_placed\", \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5c3702e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import json\n",
    "\n",
    "# Initialise LLM\n",
    "llm = ChatOpenAI(temperature=0, streaming=True)\n",
    "\n",
    "# Define AgentState\n",
    "class AgentState(dict):\n",
    "    def ___int___(self, claim_id=None, transaction_id=None, search_results=None):\n",
    "        super().___init___()\n",
    "        self[\"claim_id\"] = claim_id\n",
    "        self[\"transaction_id\"] = transaction_id\n",
    "#         self[\"search_results\"] = search_results or {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fda4d244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state):\n",
    "    claim_id = state[\"claim_id\"]\n",
    "    \n",
    "    # Fetch claim metadata to get transaction id\n",
    "    claim_metadata = index_claims.similarity_search(claim_id, k=1)\n",
    "    transaction_id = claim_metadata[0].metadata.get(\"transaction_id\", \"UNKNOWN\")\n",
    "    \n",
    "    state[\"transaction_id\"] = transaction_id\n",
    "    \n",
    "    return state\n",
    "\n",
    "def supervisor(state):\n",
    "    \n",
    "#     claim_id = state[\"claim_id\"]\n",
    "    \n",
    "#     # Fetch claim metadata to get transaction id\n",
    "#     claim_metadata = index_claims.similarity_search(claim_id, k=1)\n",
    "#     transaction_id = claim_metadata[0].metadata.get(\"transaction_id\", \"UNKNOWN\")\n",
    "    \n",
    "#     state[\"transaction_id\"] = transaction_id\n",
    "    \n",
    "    # Query Visa Stop Payment Service to check if Stop Payment was placed\n",
    "    stop_payment_status = index_visa_stop_payment.similarity_search(state[\"transaction_id\"], k=1)\n",
    "    stop_payment_flag = stop_payment_status[0].metadata.get(\"stop_payment_placed\", \"No\") if stop_payment_status else \"No\"\n",
    "    \n",
    "    print(\"Stop Payment Status\",stop_payment_status)\n",
    "    print(\"Stop Payment Flag\",stop_payment_flag)\n",
    "    \n",
    "    if stop_payment_flag == \"Yes\":\n",
    "        return [\"claims\", \"visa_stop_payment\", \"mainframe\"]\n",
    "    else:\n",
    "        return [\"claims\", \"mainframe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc656dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_claims(state):\n",
    "    \"\"\"Query Claims7.0 Database.\"\"\"\n",
    "    results = index_claims.similarity_search(state[\"claim_id\"], k=1)\n",
    "    return {\"search_results\": {**state[\"search_results\"], \"claims\": results}}\n",
    "\n",
    "def query_visa_stop_payment(state):\n",
    "    \"\"\"Query Visa Stop Payment Service Database.\"\"\"\n",
    "    results = index_visa_stop_payment.similarity_search(state[\"transaction_id\"], k=1)\n",
    "    return {\"search_results\": {**state[\"search_results\"], \"visa_stop_payment\": results}}\n",
    "\n",
    "def query_mainframe(state):\n",
    "    \"\"\"Query Mainframe Transaction Database.\"\"\"\n",
    "    results = index_mainframe.similarity_search(state[\"transaction_id\"], k=1)\n",
    "    return {\"search_results\": {**state[\"search_results\"], \"mainframe\": results}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a80e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregator(state):\n",
    "    \"\"\"Formats and summarizes stop payment validation results.\"\"\"\n",
    "    all_results = state[\"search_results\"]\n",
    "\n",
    "    structured_response = {\n",
    "        \"Claim ID\": state[\"claim_id\"],\n",
    "        \"Transaction ID\": state[\"transaction_id\"],\n",
    "        \"Claims Details\": all_results.get(\"claims\", []),\n",
    "        \"Visa Stop Payment Confirmation\": all_results.get(\"visa_stop_payment\", []),\n",
    "        \"Credit Card Transaction Data\": all_results.get(\"mainframe\", [])\n",
    "    }\n",
    "\n",
    "    summary_prompt = f\"Summarize and validate stop payment claim:\\n{json.dumps(structured_response, indent=2)}\"\n",
    "    summary = llm.invoke([HumanMessage(content=summary_prompt)])\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=summary.content)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e122b8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "# Create a stateful graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"agent\", agent)\n",
    "workflow.add_node(\"supervisor\", supervisor)\n",
    "workflow.add_node(\"claims\", query_claims)\n",
    "workflow.add_node(\"visa_stop_payment\", query_visa_stop_payment)\n",
    "workflow.add_node(\"mainframe\", query_mainframe)\n",
    "workflow.add_node(\"aggregator\", aggregator)\n",
    "\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_edge(\"agent\", \"supervisor\")\n",
    "\n",
    "# Supervisor dynamically determines which data sources to query\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    lambda state: supervisor(state),\n",
    "    {\n",
    "        \"claims\": \"visa_stop_payment\",\n",
    "        \"visa_stop_payment\": \"mainframe\",\n",
    "        \"mainframe\": \"aggregator\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Aggregator is the final step\n",
    "# workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8f5bb23",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Must write to at least one of []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m AgentState(\n\u001b[0;32m      3\u001b[0m     claim_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLM002\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run the workflow with the input state\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m app\u001b[38;5;241m.\u001b[39minvoke(initial_state)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2069\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2068\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2069\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   2070\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   2071\u001b[0m     config,\n\u001b[0;32m   2072\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[0;32m   2073\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   2074\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   2075\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   2076\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[0;32m   2077\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2078\u001b[0m ):\n\u001b[0;32m   2079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2080\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:1724\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[0;32m   1718\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates.\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;66;03m# Channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[0;32m   1721\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[0;32m   1722\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps.\u001b[39;00m\n\u001b[0;32m   1723\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[1;32m-> 1724\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   1725\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m   1726\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   1727\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[0;32m   1728\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   1729\u001b[0m         ):\n\u001b[0;32m   1730\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   1731\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[0;32m   1732\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\runner.py:230\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[0;32m    228\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     run_with_retry(\n\u001b[0;32m    231\u001b[0m         t,\n\u001b[0;32m    232\u001b[0m         retry_policy,\n\u001b[0;32m    233\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m    234\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[0;32m    235\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[0;32m    236\u001b[0m         },\n\u001b[0;32m    237\u001b[0m     )\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39minvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:506\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    502\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m    503\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    504\u001b[0m )\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 506\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\utils\\runnable.py:262\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    261\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m--> 262\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    264\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\write.py:96\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     88\u001b[0m     writes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     89\u001b[0m         ChannelWriteEntry(write\u001b[38;5;241m.\u001b[39mchannel, \u001b[38;5;28minput\u001b[39m, write\u001b[38;5;241m.\u001b[39mskip_none, write\u001b[38;5;241m.\u001b[39mmapper)\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(write, ChannelWriteEntry) \u001b[38;5;129;01mand\u001b[39;00m write\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;129;01mis\u001b[39;00m PASSTHROUGH\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[0;32m     95\u001b[0m     ]\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(\n\u001b[0;32m     97\u001b[0m         config,\n\u001b[0;32m     98\u001b[0m         writes,\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_at_least_one_of \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    100\u001b[0m     )\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langgraph\\pregel\\write.py:157\u001b[0m, in \u001b[0;36mChannelWrite.do_write\u001b[1;34m(config, writes, require_at_least_one_of)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m require_at_least_one_of \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m {chan \u001b[38;5;28;01mfor\u001b[39;00m chan, _ \u001b[38;5;129;01min\u001b[39;00m tuples} \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(require_at_least_one_of):\n\u001b[1;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust write to at least one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrequire_at_least_one_of\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         )\n\u001b[0;32m    160\u001b[0m write: TYPE_SEND \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_SEND]\n\u001b[0;32m    161\u001b[0m write(tuples)\n",
      "\u001b[1;31mInvalidUpdateError\u001b[0m: Must write to at least one of []",
      "\u001b[0mDuring task with name '__start__' and id 'a5350cb7-92d7-6791-17d9-6eef4cbf7f78'"
     ]
    }
   ],
   "source": [
    "# Create an initial state with input values\n",
    "initial_state = AgentState(\n",
    "    claim_id=\"CLM002\"\n",
    ")\n",
    "\n",
    "# Run the workflow with the input state\n",
    "result = app.invoke(initial_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
